---
title: 'dialectR: Doing Dialectometry in R'
author: "Ryan Soh-Eun Shim"
abstract: dialectR is an open-source R software package that provides access to methods
  in dialectometry. While other open-source software exist for such analyses (e.g.
  L04 and Gabmap), dialectR aims to facilitate the possibility of extending these
  methods with the statistical packages available in the R ecosystem. The core of
  the computations are implemented in C++, thus ensuring reasonably fast results.
  Visual presentations common in dialectometry are also implemented (e.g. MDS
  maps of dialect continuua), which allow for modifiable graphics. Here we present
  the package by replicating an MDS-based dialect continuum plot for Dutch, using
  data from the Goeman-Taeldeman-Van Reenen-project. We also showcase newer functionalities
  not yet available in other dialectometry software, by replicating a plot of the
  acoustic vowel space with an MFCC-based acoustic distance.
output:
  pdf_document:
    highlight: monochrome
    fig_caption: true
    latex_engine: xelatex
  html_document:
    df_print: paged
csl: apa.csl
bibliography: bibliography.bib
indent: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
```{r, screenshot.force = TRUE, echo=FALSE, fig.align='center', fig.cap='Minimal dialectR workflow'}
DiagrammeR::grViz("digraph {
# initiate graph
graph [layout = dot, rankdir = LR]

# global node settings
node [shape = rectangle]

# label nodes
kmlFile [label = 'KML \n File']
transData [label = 'Transcription \n Data']
kmlData [label = 'KML \n Data']
distComp [label = 'Distance \n Matrix']
viz [label= 'Visualization', shape = diamond]
mds [label='MDS \n Map']
clust [label='Clustering \n Map']

# edge definitions with the node IDs
kmlFile -> kmlData [label='get_points \n get_polygon'];
transData -> distComp [label='distance_matrix'];
distComp -> viz
kmlData -> viz
viz -> mds [label='mds_map'];
viz -> clust [label='cluster_map'];
}")
```

Although in recent years the application of quantitative and computational methods to linguistic data has become increasingly common, in dialectology such methods have generally relied on specialized software that vary in accessibility, and generally do not lend well to easy extension [@leinonenUsingGabmap2016]. dialectR seeks to fill this gap by providing an implementation of dialectometric methods in R, which previously have required software such as RuG/L04 and Gabmap for its analysis. In this paper, we demonstrate the functionalities of this package by following the steps in Figure 1 to replicate a visual analysis of Dutch dialects with data from the Goeman-Taeldeman-Van Reenen-project.

## Edit Distance in Dialectometry

Distance computations based on modified forms of edit distance constitute an important part of dialectometry, and stands as a first step before further analyses can be performed. We here provide a brief introduction to edit distance, along with modifications to it that have become of use in dialectometry, before delving into how it may be computed with dialectR.

Edit distance, alternatively known also as Levenshtein distance, is a method of comparing two sequences with the end goal of deriving a "distance" between the two. This distance is based on the number of insertions, deletions, and substitutions required for one string to transform into the other. As an example, consider how the string "koguma," the word for sweet potato in Korean, may be transformed into "kokoimo," the origin of the Korean term from the Tsushiba dialect in Japan, with the three operations:

```{r, echo=F, fig.align='center', warning=FALSE}
distTable <- rbind(c("k", "o", "-", "g", "u", "m", "a"), c("k", "o", "k", "o", "i", "m", "o"), c("", "", "1", "1", "1", "", "1"))
knitr::kable(distTable)
```
```{r, screenshot.force = TRUE, echo=FALSE, fig.align='center', fig.cap='Edit distance between "koguma" and "kokoimo"'}
DiagrammeR::grViz("digraph {
# initiate graph
graph [layout = dot, rankdir = LR]

# global node settings
node [shape = rectangle]

# label nodes
str1 [label = 'koguma']
str2 [label = 'kokguma']
str3 [label = 'kokouma']
str4 [label = 'kokoima']
str5 [label = 'kokoimo']
str1 -> str2 [label='Insert k'];
str2 -> str3 [label='Sub g for o'];
str3 -> str4 [label='Sub u for i'];
str4 -> str5 [label='Sub a for o'];
}")
```
We see in Figure 2 that with one insertion and three substitutions, we are able to transform the string in Korean into the string in the Japanese Tsushiba dialect. While this method may seem crude for not taking into account the degree of similarity between phonemes (e.g. 'd' should ideally be closer to 't' than to, say, 'r'), empirical studies have shown that aggregate results correlate well with human perceptions of dialect differences, and thus stands as a good, albeit approximate, probe. A simple modification in dialectometry upon this method is to set a constraint that forbids the alignment of vowels and consonants by increasing the cost of their alignment, which results in substantially better results (), and is provided in this package. The distance functions can be called as below, where the results are different because vc_leven (standing for VC-sensitive levenshtein distance) penalizes the alignment of the word final "r" and "a":
```{r, echo=FALSE, message=FALSE}
library(dialectR)
leven("vir", "via")
vc_leven("vir", "via")
```
For aggregate analyses however, there will inevitably be cases where multiple responses are given for a given dialect location, or the difference in string lengths might distort the data so as to give inaccurate results. We describe options to account for these conditions below.

### Normalization by Alignment Length
In comparing two sequences, longer sequences possess a much higher chance containing more differences than shorter sequences. If used directly, this would bias the results by causing varieties with longer sequences to appear more different. We thus follow Wilbert Heeringa in normalizing the distance by dividing the alignment length [@heeringaMeasuringDialectPronunciation2004; @heeringaEvaluationStringDistance2006], which can be done as below:
```{r}
leven("vir", "via", alignment_normalization = TRUE)
```

### Bilbao Distance for Multiple Responses
For a treatment of multiple responses in dialect data, Aurrekoetxea et al. specifies the following desiderata [@aurrekoetxeaUnifyingAnalysesMultiple2020]:

1. The frequency of a response should not matter, thus the difference between {a, a, b, c, c, c} and  {a, b, d, d} should be the same as between {a, b, c} and  {a, b, d}.
2. The distance between identical sets should be 0.
3. The procedure should be able to accommodate different cost functions, such as edit distance and inverse frequency weighting
4. It should be based on the measure of individual responses.

Aurrekoetxea et al. proposes Bilbao distance as a solution that meets the above criteria. The formula for the procedure is as below:

$$D_{B}(A,B)=\frac{\sum_{i=1}^{|A|}\displaystyle\min_{b_j\in|B|}d(a_ib_j) + \textstyle\sum_{j=1}^{|B|}\displaystyle\min_{a_j\in|A|}d(a_ib_j)}{|A| + |B|}$$
Where, in plain words, every element in a given set A is computed for a minimum distance with the elements of set B, and the same is done for all the elements in set B against those of set A. We illustrate this with an example: suppose we have {a,b} as set A and {b,c} as set B. Using edit distance as the distance metric, the minimum distance for *a* in set A as compared against set B is 1, and for *b* in set A it is 0 (since an identical element also exists in set B). Moving to set B, we similarly have a minimum distance of 0 for *b* and 1 for *c* when compared against set A. We add all the distances up, and divide the sum by the total number of elements, which yields:

$$D_{B}(\{a,b\},\{b,c\})=\frac{1+0+0+1}{2+2}=\frac{1}{2}$$
This procedure is implemented in the package and may be used as below, where the delimiter of the multiple responses should be specified:
```{r}
leven("a/b", "b/c", delim = "/")
vc_leven("vir/via", "via/jura", delim = "/")
```

## Visual Presentation of Variation
With the modified edit distance function above, we demonstrate how an aggregate analysis of dialect differences may be carried out with the package. We use data from the Goeman-Taeldeman-Van Reenen-project as an example, since many analyses in dialectometry have been done on this, and may serve as a backdrop against which to compare the results of this package. The data from the project may be loaded as below. Note however that the data must be in a format where the rows represent the data collections sites and the columns the data item types, which is the required format for Gabmap as well. We show a sample of this below:
```{r}
load("C:\\Users\\USER\\Documents\\R_packages\\dialectR\\data\\Dutch.rda")
Dutch[1:3, 1:2]
```

A distance matrix as computed with the modified edit distance discussed above may be obtained as below, where an additional option to choose either a VC-sensitive edit distance or a plain one is possible:
```{r}
distDutch <- distance_matrix(Dutch[1:50, 1:50], "vc_leven", alignment_normalization = TRUE)
distDutch[1:3,1:3]
```

### Multidimensional Scaling
With the results obtained above, we are now at a position to reproduce a map of the Dutch dialect continuum as shown in (XXX). In essence, this procedure performs classical multidimensional scaling on the distance matrix to reduce the dimensions to three; the three dimensions are then mapped to RGB values respectively, mixed together, and then projected onto a map. The function and the result is as below:
```{r, eval=FALSE}
mds_map(distDutch)
```
![](C:/Users/USER/Downloads/dutch_mds.png)

### Clustering
We also provide functions for hierarchical clustering which, as it is based on R's native hclust method (XXX), allows for a range of parameters, most importantly including the choice of the agglomeration method. For instance, it is possible to obtain maps computed by the weighted average and Ward's method, as shown below:
```{r, eval=FALSE}
cluster_map(distDutch)
cluster_map(distDutch)
```

## Extending dialectR

```{r}
dfDutch <- distmat_to_df(distDutch)
dfDutch[1:5,]
```

```{r, message=FALSE}
library(dplyr)
dfDutch %>%
  filter(row == "Aalsmeer NH")
  # ggplot for ref point and beammap
```

## MFCC-based Acoustic Distance
While dialectR has followed Gabmap in implementing many of the functionalities, we have additionally made available an acoustic distance published by Martijn Bartelds et al. in a recent paper, which makes it possible to perform the similar analyses as those based on edit distance, without having to go through the effort of transcription. While we refer the reader to the paper for the technical details, we briefly remark that the distance essentially performs dynamic time warping upon audio files, which are represented as Mel-frequency cepstral coefficients (henceforth MFCC). We illustrate its use by calculating the distance between vowels in typical vowel chart, in an attempt to reproduce in this manner the acoustic vowel space. This experiment has been succesfully attempted a number of times, both in the paper by Bartelds et al. and in dialectometry in general (), and thus again may serve as a good backdrop of comparison.

For the data, we make use of (cardinal?) vowels as pronounced by Peter Ladefoged (), which we refer the reader to download directly from the website if she wishes to reproduce the results. Assuming all the vowels were in a single folder however, it is possible to obtain a distance matrix with a nested loop, as shown below:

```{r}
vowel_dist <- sapply(1:12, function(x){
  sapply(1:12, function(y){
  acoustic_distance(list.files("C:/Users/USER/Downloads/ipa_vowels", full.names = TRUE)[x],
                    list.files("C:/Users/USER/Downloads/ipa_vowels", full.names = TRUE)[y])
  })
})

vowel_names <- c("ɛ", "a", "e", "i", "ɪ", "o", "ɑ", "ɔ", "ø", "u", "y", "ʏ")
row.names(vowel_dist) <- vowel_names
colnames(vowel_dist) <- vowel_names

vowel_dist
```

With the distance matrix, we perform multidimensional scaling to reduce the dimensions, and plot the first two as the x and y axes:

```{r, fig.width=7, fig.height=7, fig.align='center'}
vowel_mds <- cmdscale(vowel_dist, k = 3)

plot(-vowel_mds[,2],
     vowel_mds[,1])

text(-vowel_mds[,2],
     vowel_mds[,1],
     cex=0.8,
     labels = vowel_names, pos = 4)
```
Which would, on the whole, appear to be consistent with a formant-based acoustic vowel space, as confirmed by the results in the original paper.


## Conclusion
dialectR sets out to fill a unique gap in dialectology research, where with the growing number of linguists with computational training, it is reasonable to assume a level of technical competence that was previously not possible. As such, this package can be seen as a preliminary answer to some of the self-proclaimed shortcomings of Gabmap, where " . . . the most 
important . . . would involve making it easier for others to contribute modules, 
i.e. adopting an open-source development mode. Once it becomes easier for others to 
contribute, then the scientific imagination is the limiting factor" [???]. As dialectR remains in active development at this stage however, many possible improvements do also remain. An important one among these is the addition of the pointwise-mutual-information-based edit distance [???], which has been reported to generate significantly better alignments on the one hand [???], and being able to remedy differences in transcription on the other [???]. The point remains however, that with the open-source nature of the project, such additions should become all the more viable.

## References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent